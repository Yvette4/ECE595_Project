{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECE 595 Project - Unsupervised Multi-modal Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project is based on the paper below:\n",
    "\n",
    "The model is composed of the following modules:\n",
    "- 2 transformer encoders\n",
    "- 2 transformer decoders\n",
    "- 1 image encoder\n",
    "\n",
    "This implementation will only look at the English to German translations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "#import pandas as pd\n",
    "import fasttext\n",
    "import io\n",
    "from tensorflow.keras.applications import ResNet152\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Training and Preprocessing\n",
    "The Multi30k and WMT News Crawl datasets were used in the paper. To prevent the model from seeing any paired sentence information, the Multi30k training and testing datasets were split in half, one for English, and the other for German. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-training embeddings using fastText\n",
    "Pre-training of the model was implemented following what was done in [reference 20 of paper]. The original paper used the first 10 million sentences from the 2007 - 2019 datasets for each language, but due to limited processing resources, this implementation only used the 2018 German and 2019 English datasets. The embeddings were generated using fasttext. The training was executed once and stored for later access.\n",
    "\n",
    "Reference: https://github.com/facebookresearch/fastText/blob/master/python/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# Pre-training the model using News Crawl 2018 and 2019 data\n",
    "englishDataPath = './DataFiles/TRAIN_news.2019.en.shuffled.deduped'\n",
    "germanDataPath  = './DataFiles/TRAIN_news.2018.de.shuffled.deduped'\n",
    "\n",
    "# train model to get the attenuation inputs\n",
    "# paper provided parameters: dimension = 512, window size = 5, negative samples = 10\n",
    "#englishModel = fasttext.train_unsupervised(englishDataPath, model='skipgram', dim=512, ws=5, neg=10)\n",
    "#englishModel.save_model(\"englishModel2019.bin\")\n",
    "#germanModel = fasttext.train_unsupervised(germanDataPath, model='skipgram', dim=512, ws=5, neg=10)\n",
    "#germanModel.save_model(\"germanModel2018.bin\")\n",
    "\n",
    "englishModel = fasttext.load_model(\"./GeneratedFiles/englishModel2019.bin\")\n",
    "germanModel  = fasttext.load_model(\"./GeneratedFiles/germanModel2018.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283843\n"
     ]
    }
   ],
   "source": [
    "# create an embedding matrix that will be used to initialize the model\n",
    "# will be storing all vectors associated with the traied words\n",
    "enSize = len(englishModel.get_words())\n",
    "deSize = len(germanModel.get_words())\n",
    "\n",
    "enInitEmbeddings = englishModel.get_output_matrix()\n",
    "deInitEmbeddings = germanModel.get_output_matrix()\n",
    "\n",
    "print(len(enInitEmbeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes the input file and tokenizes the input sentences\n",
    "def tokenizer(sentence):\n",
    "    textTokenizer = Tokenizer()\n",
    "    \n",
    "    # Updates internal vocabulary based on the input.\n",
    "    textTokenizer.fit_on_texts(sentence)\n",
    "    \n",
    "    # Transforms each sequence into a list of text.\n",
    "    token = textTokenizer.texts_to_sequences(sentence)\n",
    "    token = tf.keras.preprocessing.sequence.pad_sequences(token, padding='post')\n",
    "    \n",
    "    return token, textTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in a text file and returns arrays with indexes into the tokens and token objects\n",
    "def load_dataset(path):\n",
    "    \n",
    "    fileData = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "    tokenInfo = []\n",
    "    tokenArray = []\n",
    "    \n",
    "    token, textTokenizer = tokenizer(fileData)\n",
    "    \n",
    "#    token = tf.transpose(token)\n",
    "#     tokenObjectArray = []\n",
    "#     yvette = 0\n",
    "    \n",
    "#     for line in fileData:\n",
    "#         yvette +=1\n",
    "#         token, textTokenizer = tokenizer(line)\n",
    "#         token = tf.transpose(token)\n",
    "#         #tokenInfo.append([token, textTokenizer])\n",
    "#         #tokenArray.append(token)\n",
    "#         #tokenObjectArray.append(textTokenizer)\n",
    "    \n",
    "    return token, textTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN Shape:  (1000, 41)\n",
      "DE Shape:  (1000, 39)\n"
     ]
    }
   ],
   "source": [
    "enFilePath2007 = './DataFiles/en2007_reduced_DataCrawl.txt'\n",
    "deFilePath2007 = './DataFiles/de2007_reduced_DataCrawl.txt'\n",
    "#enSmall = './DataFiles/enSmall_2007_DataCrawl.txt'\n",
    "#deSmall = './DataFiles/deSmall_2007_DataCrawl.txt'\n",
    "enSmall = './DataFiles/test_2017_flickr.en'\n",
    "deSmall = './DataFiles/test_2017_flickr.de'\n",
    "\n",
    "# obtain the tokenized version of the input files\n",
    "en2007_token, en2007_token_object = load_dataset(enSmall)\n",
    "de2007_token, de2007_token_object = load_dataset(deSmall)\n",
    "\n",
    "# print out stuff just to check valid inputs\n",
    "print('EN Shape: ', en2007_token.shape)\n",
    "print('DE Shape: ', de2007_token.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementation\n",
    "The entire model is composed of the following transformer modules:\n",
    "- English Encoder\n",
    "- German Encoder\n",
    "- Image Encoder\n",
    "- English Decoder\n",
    "- German Decoder\n",
    "\n",
    "The transformer model taken from tensorflow was modified to account for the pretrained embeddings previously generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet-152 Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageModel = ResNet152()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocess_input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-159-d890ac83b131>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#imgBatch = np.expand_dim(imgArray, axis=0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#imgPreProc = tf.keras.preprocessing.image.preprocess_input(image_batch,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mimgPreProc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# x = preprocess(img)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocess_input' is not defined"
     ]
    }
   ],
   "source": [
    "img = tf.keras.preprocessing.image.load_img('./images/22439193117_4183a49ec2.jpg', target_size=(224, 224))\n",
    "imgArray = tf.keras.preprocessing.image.img_to_array(img)\n",
    "#imgBatch = np.expand_dim(imgArray, axis=0)\n",
    "#imgPreProc = tf.keras.preprocessing.image.preprocess_input(image_batch, \n",
    "imgPreProc = preprocess_input(imgArray)\n",
    "\n",
    "# x = preprocess(img)\n",
    "y = imageModel.predict(imgPreProc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Queries, Keys, and Values\n",
    "The embeddings for each word are used to create the queries, keys, and values vectors that will be needed when calculating attention. These vectors are created by multiplying the embedding by three matricies that were trained during the training process. \n",
    "\n",
    "reference: https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04\n",
    "Q - vector representation of one word in the sequence\n",
    "K - vector representation of all words in the sequence\n",
    "V - vector representation of all words in the sequence\n",
    "Q is a matrix that contains the query (vector representation of one word in the sequence),\n",
    " K are all the keys (vector representations of all the words in the sequence) and V are the values, which are again the vector representations of all the words in the sequence. \n",
    " \n",
    " $ Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{dk}}) V $\n",
    " \n",
    "Transformer taken from Tensorflow help guides.\n",
    "Referenced https://www.tensorflow.org/tutorials/text/transformer#top_of_page\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate parameters for the transformer\n",
    "layers      = 4\n",
    "dimension   = 512\n",
    "dff         = 2018\n",
    "numHeads    = 8\n",
    "#enVocabSize = en2007_token.shape[1]\n",
    "#deVocabSize = de2007_token.shape[1]\n",
    "enVocabSize = 8500\n",
    "deVocabSize = 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 39, 8000)\n"
     ]
    }
   ],
   "source": [
    "enTransformer = Transformer(\n",
    "                        num_layers=layers, \n",
    "                        d_model=dimension, \n",
    "                        num_heads=numHeads, \n",
    "                        dff=dff, \n",
    "                        input_vocab_size=enVocabSize,\n",
    "                        target_vocab_size=deVocabSize,\n",
    "                        pe_input=10000, # tensorflow values used\n",
    "                        pe_target=6000) # tensorflow values used\n",
    "                        #initEmbeddings=enInitEmbeddings)\n",
    "\n",
    "enOut, enWeights = enTransformer(en2007_token, de2007_token, training=False, \n",
    "                               enc_padding_mask=None, \n",
    "                               look_ahead_mask=None,\n",
    "                               dec_padding_mask=None)\n",
    "\n",
    "print(enOut.shape)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "enTransformer.compile(optimizer=optimizer, loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam() # Yvette - 'SGD' ...?\n",
    "crossEntropyLoss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "# passing in the target language\n",
    "def lossFcn(real, prediction):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    mask = tf.cast(mask, dtype=tf.int64)\n",
    "    loss = crossEntropyLoss(real, prediction, sample_weight=mask)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# YVETTE - what is the difference between tarLangIn and tarLangOut in our dataset...?\n",
    "def trainStep(inLang, tarLangIn, tarLangOut):\n",
    "    encoderPadMask, combinedPadMask, decoderPadMask = create_masks(inLang, tarLang)\n",
    "    \n",
    "    with tf.GradientTape() as tape: \n",
    "        prediction, predictionWeights = enTransformer(inLang, tarLang, training=True, \n",
    "                                                     enc_padding_mask=encoderPadMask, \n",
    "                                                     look_ahead_mask=combinedPadMask,\n",
    "                                                     dec_padding_mask=decoderPadMask)\n",
    "        \n",
    "        loss = lossFcn(inLang, tarLangOut)\n",
    "        \n",
    "        gradients = tape.gradient(loss, enTransformer.trainable_variables)    \n",
    "        optimizer.apply_gradients(zip(gradients, enTransformer.trainable_variables))\n",
    "\n",
    "    return loss\n",
    "\n",
    "def predict(sentence):\n",
    "    # start with tokenizing the input to test\n",
    "    textTokenizer = Tokenizer()\n",
    "    inputSentence = texts_to_sequences(sentence)\n",
    "    \n",
    "    # pass through the transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow's Transformer Model\n",
    "NOTE: FOLLOWING CODE IS NOT MINE!!! Came from the site below: \n",
    "https://www.tensorflow.org/tutorials/text/transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding and Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "  return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "  # apply sin to even indices in the array; 2i\n",
    "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "  # apply cos to odd indices in the array; 2i+1\n",
    "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "  pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "\n",
    "  # add extra dimensions to add the padding\n",
    "  # to the attention logits.\n",
    "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "  return mask  # (seq_len, seq_len)\n",
    "\n",
    "def create_masks(inp, tar):\n",
    "  # Encoder padding mask\n",
    "  enc_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "  # Used in the 2nd attention block in the decoder.\n",
    "  # This padding mask is used to mask the encoder outputs.\n",
    "  dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "  # Used in the 1st attention block in the decoder.\n",
    "  # It is used to pad and mask future tokens in the input received by \n",
    "  # the decoder.\n",
    "  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "  dec_target_padding_mask = create_padding_mask(tar)\n",
    "  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "  return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attenuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "  \"\"\"Calculate the attention weights.\n",
    "  q, k, v must have matching leading dimensions.\n",
    "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "  The mask has different shapes depending on its type(padding or look ahead) \n",
    "  but it must be broadcastable for addition.\n",
    "\n",
    "  Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "  Returns:\n",
    "    output, attention_weights\n",
    "  \"\"\"\n",
    "\n",
    "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "  # scale matmul_qk\n",
    "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "  # add the mask to the scaled tensor.\n",
    "  if mask is not None:\n",
    "    scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "  # add up to 1.\n",
    "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "  return output, attention_weights\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "\n",
    "    assert d_model % self.num_heads == 0\n",
    "\n",
    "    self.depth = d_model // self.num_heads\n",
    "\n",
    "    self.wq = tf.keras.layers.Dense(d_model)\n",
    "    self.wk = tf.keras.layers.Dense(d_model)\n",
    "    self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "  def call(self, v, k, q, mask):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "\n",
    "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "        q, k, v, mask)\n",
    "\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "    concat_attention = tf.reshape(scaled_attention, \n",
    "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "  return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(EncoderLayer, self).__init__()\n",
    "\n",
    "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "  def call(self, x, training, mask):\n",
    "\n",
    "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "    attn_output = self.dropout1(attn_output, training=training)\n",
    "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "    ffn_output = self.dropout2(ffn_output, training=training)\n",
    "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "    return out2\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "    super(Encoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "    self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                            self.d_model)\n",
    "\n",
    "\n",
    "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "\n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "  def call(self, x, training, mask):\n",
    "\n",
    "    seq_len = tf.shape(x)[1]\n",
    "\n",
    "    # adding embedding and position encoding.\n",
    "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "    x = self.dropout(x, training=training)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "    return x  # (batch_size, input_seq_len, d_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "\n",
    "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "\n",
    "  def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "    attn1 = self.dropout1(attn1, training=training)\n",
    "    out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "    attn2, attn_weights_block2 = self.mha2(\n",
    "        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "    attn2 = self.dropout2(attn2, training=training)\n",
    "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "    ffn_output = self.dropout3(ffn_output, training=training)\n",
    "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "    return out3, attn_weights_block1, attn_weights_block2\n",
    "\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                       for _ in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "  def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    attention_weights = {}\n",
    "\n",
    "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "    x = self.dropout(x, training=training)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                             look_ahead_mask, padding_mask)\n",
    "\n",
    "      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "\n",
    "    # x.shape == (batch_size, target_seq_len, d_model)\n",
    "    return x, attention_weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "    super(Transformer, self).__init__()\n",
    "\n",
    "    self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                           input_vocab_size, pe_input, rate)\n",
    "\n",
    "    self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                           target_vocab_size, pe_target, rate)\n",
    "\n",
    "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "  def call(self, inp, tar, training, enc_padding_mask, \n",
    "           look_ahead_mask, dec_padding_mask):\n",
    "\n",
    "    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "    dec_output, attention_weights = self.decoder(\n",
    "        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "    return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
